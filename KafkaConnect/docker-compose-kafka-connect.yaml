# version: '3.5'

networks:
  hadoop_cluster:
    name: hadoop_cluster
    driver: bridge

services:
  namenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    ports:
      - "50071:50070"
    hostname: namenode
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command: namenode

  secondarynamenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    depends_on:
      namenode:
        condition: service_healthy
        restart: true
    ports:
      - "50091:50090"
    hostname: secondarynamenode
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command: secondarynamenode

  resourcemanager:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    depends_on:
      namenode:
        condition: service_healthy
        restart: true
    ports:
      - "18088:8088"
    hostname: resourcemanager
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command: resourcemanager

  historyserver:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    depends_on:
      namenode:
        condition: service_healthy
        restart: true
      resourcemanager:
        condition: service_healthy
        restart: true
    ports:
      - "19889:19888"
    hostname: historyserver
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command: historyserver

  mysql:
    image: ashokkumarchoppadandi/mysql:latest
    networks:
      - hadoop_cluster
    hostname: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: Password@123
      MYSQL_DATABASE: metastore
    healthcheck:
      test: mysqladmin ping -h 127.0.0.1 -u root --password=$$MYSQL_ROOT_PASSWORD
      interval: 10s
      timeout: 10s
      retries: 3
      start_period: 30s

  hiveserver2:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    depends_on:
      namenode:
        condition: service_healthy
        restart: true
      resourcemanager:
        condition: service_healthy
        restart: true
      historyserver:
        condition: service_healthy
        restart: true
      mysql:
        condition: service_healthy
        restart: true
    ports:
      - "10002:10002"
    hostname: hiveserver2
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command:
      - hiveserver2

  postgres:
    image: postgres:13
    container_name: postgres
    hostname: postgres
    networks:
      - hadoop_cluster
    environment:
      POSTGRES_DB: kafkadb
      POSTGRES_USER: hadoop
      POSTGRES_PASSWORD: Password@123
    ports:
      - "5432:5432"

  slavenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8-stream8
    networks:
      - hadoop_cluster
    depends_on:
      namenode:
        condition: service_healthy
        restart: true
      secondarynamenode:
        condition: service_healthy
        restart: true
      resourcemanager:
        condition: service_healthy
        restart: true
      historyserver:
        condition: service_healthy
        restart: true
      hiveserver2:
        condition: service_healthy
        restart: true
    ports:
      - "18042:8042"
      - "35945:35945"
      - "4040:4040"
    hostname: slavenode
    restart: always
    environment:
      FS_DEFAULT_NAME: hdfs://namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql://mysql/metastore?createDatabaseIfNotExist=true
    command:
      - slavenode

  zookeeper:
    image: ashokkumarchoppadandi/confluent-kafka:6.2.0-stream8-with-connectors
    networks:
      - hadoop_cluster
    ports:
      - "2181:2181"
    hostname: zookeeper
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_MAX_CLIENT_CONNECTIONS: 0
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_ENABLE_SERVER: "false"
      ZOOKEEPER_ADMIN_SERVER_PORT: 8080
    command:
      - zookeeper
      - standalone

  broker:
    image: ashokkumarchoppadandi/confluent-kafka:6.2.0-stream8-with-connectors
    networks:
      - hadoop_cluster
    depends_on:
      zookeeper:
        condition: service_healthy
        restart: true
    hostname: broker
    restart: always
    environment:
      BROKER_ID: 0
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:9092,EXTERNAL://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      NUM_NETWORK_THREADS: 3
      NUM_IO_THREADS: 8
      SOCKET_SEND_BUFFER_BYTES: 102400
      SOCKET_RECEIVE_BUFFER_BYTES: 102400
      SOCKET_REQUEST_MAX_BYTES: 104857600
      NUM_PARTITIONS: 1
      NUM_RECOVERY_THREADS_PER_DATA_DIR: 1
      OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      TRANSACTION_STATE_LOG_MIN_ISR: 1
      LOG_RETENTION_HOURS: 168
      LOG_SEGMENT_BYTES: 1073741824
      LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      ZOOKEEPER_CONNECT_LIST: "zookeeper:2181"
      ZOOKEEPER_CONNECTION_TIMEOUT_MS: 18000
      GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    ports:
      - "29092:29092"
    command: kafka

  schemaregistry:
    image: ashokkumarchoppadandi/confluent-kafka:6.2.0-stream8-with-connectors
    networks:
      - hadoop_cluster
    depends_on:
      broker:
        condition: service_healthy
        restart: true
    hostname: schemaregistry
    restart: always
    environment:
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      KAFKASTORE_BOOTSTRAP_SERVERS: "broker:9092"
      KAFKASTORE_TOPIC: "_schemas"
      DEBUG: "true"
    ports:
      - "8081:8081"
    command: schemaregistry

  connect:
    image: ashokkumarchoppadandi/confluent-kafka:6.2.0-stream8-with-connectors
    networks:
      - hadoop_cluster
    depends_on:
      broker:
        condition: service_healthy
        restart: true
      schemaregistry:
        condition: service_healthy
        restart: true
    hostname: connect
    restart: always
    environment:
      CONNECT_WITH_AVRO_DISTRIBUTED: "true"
      BOOTSTRAP_SERVERS: broker:9092
      GROUP_ID: connect-cluster
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schemaregistry:8081
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schemaregistry:8081
      KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      OFFSET_STORAGE_TOPIC: connect-offsets
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_PARTITIONS: 25
      CONFIG_STORAGE_TOPIC: connect-configs
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_TOPIC: connect-status
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_PARTITIONS: 5
      OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_REST_HOST: 0.0.0.0
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST: connect
      CONNECT_REST_ADVERTISED_PORT: 8083
    ports:
      - "8083:8083"
    command: connect

  ksql:
    image: ashokkumarchoppadandi/confluent-kafka:6.2.0-stream8-with-connectors
    networks:
      - hadoop_cluster
    depends_on:
      broker:
        condition: service_healthy
        restart: true
      schemaregistry:
        condition: service_healthy
        restart: true
    hostname: ksql
    restart: always
    environment:
      BOOTSTRAP_SERVERS: "broker:9092"
      KSQL_DB_LISTENERS: http://0.0.0.0:8088
      KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_LOGGING_PROCESSING_ROWS_INCLUDE: "true"
      KSQL_SCHEMA_REGISTRY_URL: http://schemaregistry:8081
      UI_ENABLED: "true"
      KSQL_STREAMS_AUTO_OFFSET_RESET: latest
      KSQL_STREAMS_COMMIT_INTERVAL_MS: 2000
      KSQL_STREAMS_CACHE_MAX_BYTES_BUFFERING: 10000000
      KSQL_FAIL_ON_DESERIALIZATION_ERROR: "true"
      KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_SERVICE_ID: default_
      KSQL_SINK_PARTITIONS: 4
      KSQL_SINK_REPLICAS: 1
    ports:
      - "8089:8088"
    command: ksql

  edgar_logs_producer:
    image: ashokkumarchoppadandi/edgar-logs-kafka-producer:1.0-SNAPSHOT
    hostname: edgar_logs_producer
    container_name: edgar_logs_producer
    networks:
      - hadoop_cluster
    depends_on:
      - broker
      - schemaregistry
    environment:
      BOOTSTRAP_SERVERS: broker:9092
      ACKS: all
      RETRIES: 1
      BATCH_SIZE: 16384
      LINGER_MS: 1
      BUFFER_MEMORY: 33554432
      KEY_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      VALUE_SERIALIZER: io.confluent.kafka.serializers.KafkaAvroSerializer
      SCHEMA_REGISTRY_URL: http:\/\/schemaregistry:8081
      SCHEMA_FILE_LOCATION: \/edgar\/edgar.avsc
      OUTPUT_FORMAT: avro
      TOPIC_NAME: logs-test
      INPUT_LOGS_DIR: \/edgar\/input_logs\/
      IS_CONFIGURING_PROPERTIES_REQUIRED: "yes"
      IS_KERBEROS_AUTHENTICATION_REQUIRED: "no"
      JAR_FILE_LOCATION: /java_examples/
      JAR_NAME: KafkaExamples-1.0-SNAPSHOT.jar
      RUN_CLASS: com.bigdata.kafka.producer.file.dynamic.EdgarLogsKafkaDynamicFileProducer
    volumes:
      - ~/input_logs/:/edgar/input_logs/:rw

  elasticsearch01:
    image: ashokkumarchoppadandi/elasticsearch:7.17.0
    hostname: elasticsearch01
    container_name: elasticsearch01
    networks:
      - hadoop_cluster
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      HOSTNAME: elasticsearch01
      ES_CLUSTER_NAME: my-elasticsearch
      ES_NODE_NAME: elasticsearch01
      ES_NODE_MASTER: "true"
      ES_DATA_PATH: "/usr/local/elasticsearch/data/"
      ES_LOGS_PATH: "/usr/local/elasticsearch/logs/"
      ES_NETWORK_HOST: 0.0.0.0
      ES_HTTP_PORT: 9200
      ES_BOOTSTRAP_MEMORY_LOCK: "true"
      ES_DISCOVERY_SEED_HOSTS: "127.0.0.1, [::1]"
      ES_MASTER_NODES: elasticsearch01
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1

  kibana01:
    image: ashokkumarchoppadandi/kibana:7.17.0
    hostname: kibanahost1
    container_name: kibanahost1
    depends_on:
      - elasticsearch01
    networks:
      - hadoop_cluster
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch01:9200"
      KIBANA_SERVER_PORT: 5601
      KIBANA_SERVER_HOST: 0.0.0.0
      KIBANA_SERVER_NAME: kibanahost1
